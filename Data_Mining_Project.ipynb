{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6ZmLX__z885x",
        "wL4_jp9x9QXh",
        "-9SUGZh79UaX",
        "-YzkFCEx9ZJp",
        "-eR5d-0k9cKX"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlinaniS/Data_Mining_Judgement_Topic_Classification/blob/main/Data_Mining_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of types of topics associated with judgements using text of judgement."
      ],
      "metadata": {
        "id": "0iOmZGlT7aEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding"
      ],
      "metadata": {
        "id": "9oZPpVfu-9Jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General and Specific Objectives"
      ],
      "metadata": {
        "id": "qN7thLdxAAtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Understanding\n",
        "\n",
        "The primary business objective is to automate the classification of legal judgments by topic. This addresses the significant inefficiency and inconsistency of manual classification, which is a key bottleneck in legal information management. By developing an accurate and automated system, the project aims to:\n",
        "\n",
        "General objectives:\n",
        "1.\tDefine the overall business problem or opportunity that data mining will address: The core problem is the inefficiency and subjectivity of manually classifying legal judgments by topic. This project aims to address this by developing an automated system capable of classifying types of topics present in judicial case judgements into predefined topic categories (e.g., \"Criminal Law,\" \"Family Law,\" \"Contract Disputes\") based on their textual content thereby enabling faster information retrieval, improved legal research efficiency, and enhanced decision-making support for legal practitioners, researchers, and policymakers.\n",
        "\n",
        "2.\tTo understand the business context: Legal professionals, researchers, and the public often need to search for judgments based on specific legal topics (e.g., contract law, criminal law, intellectual property). Manually tagging these judgments is a time-consuming and often inconsistent process. By applying text mining and classification techniques, the project seeks to address the need for a scalable and accurate topic classification method which will enable faster access to relevant case law and support data-driven legal analytics.\n",
        "\n",
        "\n",
        "Specific objectives:\n",
        "1.\tTranslate business problems into data mining tasks: Convert the need for topic classification into a supervised text classification task, leveraging NLP techniques (e.g., TF-IDF, word embeddings) and ML models (e.g., Super Vector Machines (SVM), BERT).\n",
        "\n",
        "2.\tAlign objectives with measurable business outcomes:\n",
        "I.\tIncreased efficiency: Reduce the time and human effort required to classify a new judgment by a certain percentage (e.g., 86%)\n",
        "II.\tImproved consistency: Achieve a higher inter-rater reliability score compared to manual classification.\n",
        "III.\tEnhanced searchability: Enable more precise and faster searching of legal databases by topic, leading to a quantifiable reduction in search time for legal professionals.\n",
        "\n",
        "3.\tRequirements elicitation to determine customer needs:\n",
        "I.\tInterview legal stakeholders to identify:\n",
        "II.\tRequired topic categories (e.g., \"Intellectual Property,\" \"Labor Disputes\").\n",
        "III.\tAcceptable error rates (e.g., 95% accuracy for high-priority categories).\n",
        "IV.\tIntegration needs (e.g., compatibility with legal databases like Westlaw or JSTOR).\n",
        "\n",
        "4.\tIdentify factors influencing project outcomes:\n",
        "I.\tData quality and availability: The success of the project is highly dependent on having a large, diverse, and well-labeled dataset of judgments\n",
        "II.\tComplexity of legal language: The nuanced and technical nature of legal language can pose a challenge to classification models.\n",
        "III.\tComputational resources: Training complex deep learning models will require significant computational power.\n",
        "IV.\tExpert input: The need for domain experts (legal professionals) to help with data labeling and model validation.\n",
        "\n",
        "\n",
        "5.\tPhase Outcomes\n",
        "\n",
        "•   Phase 1 (Business Understanding & Data Understanding): A clear project plan, identified data sources, and an initial understanding of the data's quality and characteristics.\n",
        "\n",
        "•   Phase 2 (Data Preparation): A clean and processed dataset ready for modelling.\n",
        "\n",
        "•   Phase 3 (Modelling): A trained and evaluated classification model.\n",
        "\n",
        "•   Phase 4 (Evaluation & Deployment): A final report on the model's performance and a plan for integration into a real-world system.\n",
        "\n",
        "\n",
        "Core business objectives:\n",
        "\n",
        "•\tTo create a valuable tool that can be used to improve access to and analysis of legal data.\n",
        "\n",
        "•\tReduce manual effort in legal document management.\n",
        "\n",
        "•\tTo improve the quality and consistency of legal information classification.\n",
        "\n",
        "Background information:\n",
        "\n",
        "• The project builds upon existing work in natural language processing (NLP) and text mining, specifically in the legal domain. It leverages advances in deep learning for text analysis to tackle a long-standing problem in legal information management.\n",
        "\n",
        "Key success criteria—how will the relative success of the project be measured?\n",
        "\n",
        "The project will be considered successful if it achieves an accuracy of ≥ 86% (with balanced precision, recall, and F1-score), reduces manual review time by 75% (from ~20 minutes to ≤ 5 minutes per judgement), attains ≥ 80% positive feedback from legal professionals, and can scale to process large volumes of judgements without performance loss.\n"
      ],
      "metadata": {
        "id": "drXHiM1Y8beB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jonathan, Please add your text here."
      ],
      "metadata": {
        "id": "nA1ZIxxHAO8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Situational analysis"
      ],
      "metadata": {
        "id": "aaOP4hhg_6FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section provides a detailed assessment of the current state of the project, including a review of available resources, constraints, and a preliminary cost-benefit analysis. This analysis is crucial for determining the feasibility and scope of the project.\n",
        "\n",
        "---\n",
        "\n",
        "1.3.1. Resources Available\n",
        "\n",
        "**Data**\n",
        "- Primary source: ZambiaLII (access to thousands of judicial judgments).\n",
        "- Current collection: 8,871 judgments stored as large scanned PDFs.\n",
        "- Key constraint: documents are image-based (not text), so OCR is required (e.g., Tesseract or cloud OCR services) to extract text.\n",
        "- Labeling constraint: no pre-existing topic labels — labels must be created manually or inferred from metadata (titles, citations).\n",
        "- Legal/access note: ZambiaLII is open access, so there are no financial or legal barriers to acquiring the data.\n",
        "\n",
        "**Personnel**\n",
        "- Team composition: five computer science students with data-mining background.\n",
        "- Constraint/risk: no legal domain expert on Zambian law, increasing risk of misinterpretation and mislabeling legal terms.\n",
        "- Mitigation strategy: rely on publicly available case abstracts and metadata for cross-checking; explicitly document this limitation in the project plan.\n",
        "\n",
        "**Technology**\n",
        "- Available resources: personal laptops and free cloud notebooks (e.g., Google Colab).\n",
        "- Software approach: open-source stack (Python, NLTK, spaCy, scikit-learn, etc.) to avoid licensing costs.\n",
        "- Resource constraint: limited to free Colab GPU/compute resources; dataset size (a few thousand docs) is considered manageable within those limits.\n",
        "\n",
        "**Project Requirements & Deliverables**\n",
        "- Academic requirements: deliver a fully documented codebase in Jupyter notebook format and a formal written report within the semester.\n",
        "- Data requirement: prepare a dataset of at least a few hundred to ~1,000 judgments for model training.\n",
        "- Representativeness: dataset must include judgments from different courts and time periods to reduce systemic bias.\n",
        "\n",
        "---\n",
        "\n",
        "1.3.2. Cost-Benefit Analysis\n",
        "\n",
        "**Summary**\n",
        "A preliminary cost-benefit analysis indicates the project is worthwhile: educational and prototyping benefits outweigh costs.\n",
        "\n",
        "**Costs**\n",
        "- Student effort/time: primary cost (OCR processing, manual labeling, model development).\n",
        "- Financial costs: negligible due to open-source tools and free data sources; possible but avoidable paid cloud costs if the project remains within free tiers.\n",
        "\n",
        "**Benefits**\n",
        "- Educational value: hands-on experience applying data-mining and NLP to a real problem.\n",
        "- Prototype deliverable: a functional automated legal topic classifier.\n",
        "- Long-term impact: a successful prototype could form the foundation for a larger system that improves legal research efficiency and could justify future investment.\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZNkyVbslr-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pp6mhV61GE8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broad goals of data mining process"
      ],
      "metadata": {
        "id": "-8yXoc-P_ywl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rDZJFtQc46iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broad goals of the data mining process\n",
        "\n",
        "Purpose: turn the legal team's need for topic tagging of judgements into clear technical targets, usable outputs, and success criteria so stakeholders can act on model results with confidence.\n",
        "\n",
        "1.Outputs that enable achievement of the business objectives\n",
        "\n",
        "Per-document outputs\n",
        "- Predicted topic label(s) (single-label or multi-label) + calibrated confidence scores.\n",
        "- Short human-readable explanation per prediction (top contributing phrases / feature importances / example similar cases).\n",
        "System outputs / artifacts\n",
        "- Labeled training dataset and annotation guide (taxonomic definitions, examples).\n",
        "- Evaluation report (per-class precision/recall, confusion matrix, macro/micro F1, calibration).\n",
        "- Scoring pipeline: batch job and real-time API (with SLA: latency, throughput).\n",
        "- Dashboard / report for legal teams: topic distributions, trending topics, error examples, uncertain cases for manual review.\n",
        "- Integration docs and deployment checklist for case-management/search systems.\n",
        "\n",
        "2.Technical criteria for a successful outcome (objective)\n",
        "\n",
        "Model quality\n",
        "- Target metrics (set with stakeholders): e.g., macro-F1 ≥ 0.75; per-priority-topic recall ≥ 0.80; precision thresholds for high-cost mistake classes.\n",
        "- Calibration: predicted probabilities reflect true likelihoods (Brier score or calibration curve).\n",
        "Operational\n",
        "- API latency ≤ X ms; batch throughput ≥ N docs/hour; uptime ≥ 99%.\n",
        "- Resource/compute limits acceptable for deployment environment.\n",
        "Business KPIs\n",
        "- Measurable reduction in manual labeling/review time (e.g., ≥ 50% during pilot).\n",
        "- Increase in retrieval/triage accuracy (e.g., topic-based search precision improvement).\n",
        "\n",
        "3.Subjective criteria that require human judgment\n",
        "\n",
        "Interpretability & trust\n",
        "- Legal analysts must understand explanations and accept model decisions in a sample audit (human acceptance rate target, e.g., ≥ 85%).\n",
        "Usability\n",
        "- Labels and example outputs match legal definitions and are actionable for routing/prioritization.\n",
        "Adoption\n",
        "- Stakeholder sign-off in a pilot: confidence to use model outputs without manual double-check for low-risk topics.\n",
        "\n",
        "These subjective criteria should be evaluated by named stakeholders (legal lead, adjudication manager) with clear acceptance tests (sample review protocols).\n",
        "\n",
        "4.Link outputs → criteria → business use\n",
        "\n",
        "Example mapping:\n",
        "\n",
        "Output: per-judgement topic + confidence → Use: auto-route to specialist team when confidence ≥ 0.9 → Technical success: precision ≥ 0.90 on routed cases; Business success: 60% reduction in routing time.\n",
        "\n",
        "Output: low-confidence queue → Use: human-in-loop labeling to improve rare-topic performance → Technical success: increased recall for rare topics after 2 retraining iterations.\n",
        "\n",
        "5.Risks & mitigations (brief)\n",
        "\n",
        "Risk: class imbalance / rare topics → Mitigation: active learning, oversampling, hierarchical taxonomy (coarse → fine).\n",
        "\n",
        "Risk: noisy text (OCR/redaction) → Mitigation: preprocessing & quality flags; route bad-quality docs to human review.\n",
        "\n",
        "Risk: stakeholder mistrust → Mitigation: provide explanations, confusion examples, and a pilot with feedback loop.\n",
        "\n"
      ],
      "metadata": {
        "id": "3Z0b2t0LG_2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Katrina, please add your text here."
      ],
      "metadata": {
        "id": "5Jab3QajAqVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Planning"
      ],
      "metadata": {
        "id": "x7Azl-uB_sD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Plan\n",
        "This section outlines the detailed plan for executing the data mining project, translating the business and data mining goals into a structured, time-bound, and resourced-based work breakdown. The plan is organized into four distinct phases that align with the CRISP-DM methodology.\n",
        "1.5.1. Executive Summary & High-Level Timeline\n",
        "The primary goal is to deliver an automated topic-classification pipeline for judicial judgments using supervised Natural Language Processing (NLP). The solution must meet the defined success criteria, including achieving a target accuracy of ≥86%, reducing manual review time, and securing positive stakeholder feedback. The project is structured across four phases, with a focus on iterative development to manage key constraints such as data availability and computational resources.\n",
        "Phase\tDates (2025)\tDuration (Days)\n",
        "1. Business & Data Understanding\t11 Aug – 14 Aug\t4\n",
        "2. Data Preparation\t15 Aug – 19 Aug\t5\n",
        "3. Modelling (with iterations)\t20 Aug – 24 Aug\t5\n",
        "4. Evaluation & Deployment Plan\t25 Aug – 29 Aug\t5\n",
        "1.5.2. Phase-by-Phase Plan\n",
        "Phase 1: Business & Data Understanding (11–14 Aug)\n",
        "•\tGoal: To finalize project scope, confirm requirements with stakeholders, and secure initial data access.\n",
        "•\tTasks:\n",
        "•\tDay 1 (11 Aug): Project kickoff and stakeholder interviews to define topic categories, acceptable error rates, and key performance indicators (KPIs).\n",
        "•\tDay 2 (12 Aug): Document requirements and formalize acceptance criteria.\n",
        "•\tDay 3 (13 Aug): Identify data sources (e.g., court repositories) and request access to extract an initial data sample (≈ 10-50 judgments per category).\n",
        "•\tDay 4 (14 Aug): Conduct a quick quality assessment of the data sample (e.g., OCR quality, class imbalance) and finalize the data annotation plan.\n",
        "•\tResources: Project lead, data scientist, legal subject matter expert (SME), and a data engineer.\n",
        "•\tDependencies: Timely approval for data access is a critical path item. Delays here will directly impact subsequent phases.\n",
        "Phase 2: Data Preparation (15–19 Aug)\n",
        "•\tGoal: To create a clean, annotated, and version-controlled dataset ready for modeling.\n",
        "•\tTasks:\n",
        "•\tDay 5 (15 Aug): Set up the annotation tool (e.g., Label Studio) and train labelers on the established guidelines.\n",
        "•\tDay 6-7 (16-17 Aug): Annotate a small seed dataset (≈ 200-300 judgments) and perform an inter-rater reliability check to ensure consistency.\n",
        "•\tDay 8 (18 Aug): Perform preprocessing steps including OCR cleanup, normalization, and initial feature extraction (e.g., TF-IDF).\n",
        "•\tDay 9 (19 Aug): Create a stratified train, validation, and test split, and version the final dataset.\n",
        "•\tResources: Data engineer, 1-2 labelers, and a legal SME for quality assurance.\n",
        "•\tDependencies: This phase is entirely dependent on the successful completion of data extraction in Phase 1.\n",
        "Phase 3: Modelling & Iterations (20–24 Aug)\n",
        "•\tGoal: To develop and tune a robust classification model that meets the performance criteria. This phase is designed to be iterative.\n",
        "•\tTasks:\n",
        "•\tDay 10 (20 Aug): Establish a baseline model using a simple approach (e.g., TF-IDF with SVM or Logistic Regression) and evaluate its performance.\n",
        "•\tDay 11-12 (21-22 Aug): Develop and train an advanced model, such as a fine-tuned transformer (e.g., BERT), and perform hyperparameter tuning.\n",
        "•\tDay 13-14 (23-24 Aug): Conduct two planned iterations to refine the model. This includes addressing identified weaknesses (e.g., through data augmentation, class weighting) and selecting the final model.\n",
        "•\tResources: ML engineer, data scientist, and a GPU-enabled cloud environment (e.g., Google Colab).\n",
        "•\tDependencies: This phase relies on the availability of the prepared labeled dataset from Phase 2 and sufficient computational resources (GPU access) for advanced models. A fallback plan (using simpler models like SVM) will be used if GPU access is a limiting factor.\n",
        "Phase 4: Evaluation, UAT & Deployment Plan (25–29 Aug)\n",
        "•\tGoal: To finalize model performance, validate it with end-users, and create a clear plan for system deployment and integration.\n",
        "•\tTasks:\n",
        "•\tDay 15 (25 Aug): Final evaluation of the selected model against all technical KPIs (e.g., accuracy, runtime).\n",
        "•\tDay 16 (26 Aug): Conduct User Acceptance Testing (UAT) with legal SMEs to gather feedback on usability and trust, targeting ≥80% positive feedback.\n",
        "•\tDay 17-18 (27-28 Aug): Prepare deployment artifacts (e.g., serialized model, API specifications) and create a formal integration plan for connecting the model to existing legal databases.\n",
        "•\tDay 19 (29 Aug): Final project presentation to stakeholders and formal sign-off.\n",
        "•\tResources: ML engineer, DevOps/infra specialist, legal SME, and the project lead.\n",
        "•\tDependencies: This phase is contingent on the successful completion of the modeling phase and requires active participation from stakeholders for UAT and sign-off.\n",
        "1.5.3. Risk Analysis & Mitigation\n",
        "Several risks have been identified, and a contingency strategy is in place to ensure the project stays on track.\n",
        "•\tData Access & Quality: The highest risk. A delay in securing data access or poor OCR quality could halt the project. The mitigation strategy is to request data samples immediately and work in parallel on public data to build a prototype.\n",
        "•\tCompute Availability (GPU): A high risk for advanced models. The mitigation is to reserve cloud resources in advance and have a fallback to lighter models if needed.\n",
        "•\tExpert Availability: The limited availability of legal SMEs is a medium risk. This will be managed by scheduling specific time blocks for them and using a targeted sampling approach for reviews instead of full-document checks.\n",
        "•\tContingency Strategy: A two-iteration buffer is built into the modeling phase. In case of significant delays or unmet targets, the project scope may be reduced (e.g., focusing on only the highest-priority topics) to ensure a successful outcome within the allotted timeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "2oF5NeR7AxQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding"
      ],
      "metadata": {
        "id": "6ZmLX__z885x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TEpWrOYR9PZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "wL4_jp9x9QXh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pyq3Q7r29ToO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "-9SUGZh79UaX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BO33zrd9YYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "-YzkFCEx9ZJp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FByZjyze9bm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment"
      ],
      "metadata": {
        "id": "-eR5d-0k9cKX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQq-ACP69fZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c96deac5"
      },
      "source": [
        "| Phase                                                         | Dates (2025)    | Duration (days) |\n",
        "|---------------------------------------------------------------|-----------------|-----------------|\n",
        "| Phase 1 — Business & Data Understanding                       | 11 Aug — 14 Aug | 4               |\n",
        "| Phase 2 — Data Preparation (collection, labeling, preprocessing) | 15 Aug — 19 Aug | 5               |\n",
        "| Phase 3 — Modelling (baseline → advanced; 2 iterations)       | 20 Aug — 24 Aug | 5               |\n",
        "| Phase 4 — Evaluation, UAT & Deployment plan                   | 25 Aug — 29 Aug | 5               |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0df0aca"
      },
      "source": [
        "# Display the content of the Markdown cell with the table\n",
        "with open('/content/n/notebook.ipynb', 'r') as f:\n",
        "    notebook_content = f.read()\n",
        "\n",
        "import json\n",
        "notebook_json = json.loads(notebook_content)\n",
        "\n",
        "for cell in notebook_json['cells']:\n",
        "    if cell.get('id') == 'c96deac5':\n",
        "        print(\"Content of cell c96deac5:\")\n",
        "        print(\"\".join(cell['source']))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}